var documenterSearchIndex = {"docs":
[{"location":"guide/#Buffers.jl-Usage-Guide","page":"Guide","title":"Buffers.jl Usage Guide","text":"","category":"section"},{"location":"guide/","page":"Guide","title":"Guide","text":"This guide provides detailed instructions and examples on how to use the Buffers.jl package.","category":"page"},{"location":"guide/#Creating-a-Buffer","page":"Guide","title":"Creating a Buffer","text":"","category":"section"},{"location":"guide/","page":"Guide","title":"Guide","text":"To create a buffer, you can use the Buffer type. The buffer is automatically resized based on the elements added to it.","category":"page"},{"location":"guide/","page":"Guide","title":"Guide","text":"using Buffers\n\nbuffer = Buffer{Int}() # Create a buffer of type Int","category":"page"},{"location":"guide/","page":"Guide","title":"Guide","text":"You can also specify the initial size of the buffer (number of elements) to avoid reallocation.","category":"page"},{"location":"guide/","page":"Guide","title":"Guide","text":"buffer = Buffer(100) # Create a buffer with an initial size of 100 elements","category":"page"},{"location":"guide/#Creating-a-ThreadsBuffer","page":"Guide","title":"Creating a ThreadsBuffer","text":"","category":"section"},{"location":"guide/","page":"Guide","title":"Guide","text":"The ThreadsBuffer type extends the functionality of Buffer to support multi-threaded environments. It provides thread-safe operations for concurrent data manipulation.","category":"page"},{"location":"guide/","page":"Guide","title":"Guide","text":"tbuffer = ThreadsBuffer(1000) # Create a ThreadsBuffer of type Float64","category":"page"},{"location":"guide/","page":"Guide","title":"Guide","text":"All operations on Buffer can be performed on ThreadsBuffer as well.","category":"page"},{"location":"guide/","page":"Guide","title":"Guide","text":"The buffers in ThreadsBuffer are thread-local, meaning each task uses its own buffer. This allows for concurrent allocation and deallocation of memory without causing data corruption. After the task is done, the buffer has to be released with reset! or Buffers.release! to be reused by another task.","category":"page"},{"location":"guide/#Allocation-with-alloc!","page":"Guide","title":"Allocation with alloc!","text":"","category":"section"},{"location":"guide/","page":"Guide","title":"Guide","text":"The alloc! function is used to allocate memory for tensors in the buffer.","category":"page"},{"location":"guide/","page":"Guide","title":"Guide","text":"using Buffers\n\nbuffer = Buffer(1000)\nA = alloc!(buffer, 10, 10) # Allocate a 10x10 tensor\nB = alloc!(buffer, 20, 5)  # Allocate a 20x5 tensor","category":"page"},{"location":"guide/#Deallocation-with-drop!","page":"Guide","title":"Deallocation with drop!","text":"","category":"section"},{"location":"guide/","page":"Guide","title":"Guide","text":"The drop! function is used to deallocate memory for tensors from the buffer.","category":"page"},{"location":"guide/","page":"Guide","title":"Guide","text":"drop!(buffer, A) # Deallocate tensor A\ndrop!(buffer, B) # Deallocate tensor B","category":"page"},{"location":"guide/#Resetting-the-Buffer-with-reset!","page":"Guide","title":"Resetting the Buffer with reset!","text":"","category":"section"},{"location":"guide/","page":"Guide","title":"Guide","text":"The reset! function is used to reset the buffer to its initial state, clearing all allocated memory.","category":"page"},{"location":"guide/","page":"Guide","title":"Guide","text":"reset!(buffer) # Reset the buffer","category":"page"},{"location":"guide/#Releasing-the-Buffer-with-Buffers.release!","page":"Guide","title":"Releasing the Buffer with Buffers.release!","text":"","category":"section"},{"location":"guide/","page":"Guide","title":"Guide","text":"For ThreadsBuffer, the Buffers.release! function is used to release the buffer of the current thread. Note: the reset! function includes a call to release!.","category":"page"},{"location":"guide/","page":"Guide","title":"Guide","text":"tbuffer = ThreadsBuffer(1000)\nThreads.@threads for i in 1:4\n    A = alloc!(tbuffer, 10, 10)\n    drop!(tbuffer, A)\n    Buffers.release!(tbuffer) # Release the buffer for the current thread\nend","category":"page"},{"location":"guide/#Reshaping-the-Buffer-with-reshape_buf!","page":"Guide","title":"Reshaping the Buffer with reshape_buf!","text":"","category":"section"},{"location":"guide/","page":"Guide","title":"Guide","text":"The reshape_buf! function is used to reshape the buffer to a new set of dimensions without copying the data or explicitly allocating a tensor on the buffer.","category":"page"},{"location":"guide/","page":"Guide","title":"Guide","text":"buffer = Buffer(1000)\nA = reshape_buf!(buffer, 10, 10) # Reshape buffer to a 10x10 tensor\nB = reshape_buf!(buffer, 20, 5, offset=100) # Reshape buffer to a 20x5 tensor starting at offset 100","category":"page"},{"location":"guide/#Neuralyzing-Tensors-with-neuralyze","page":"Guide","title":"Neuralyzing Tensors with neuralyze","text":"","category":"section"},{"location":"guide/","page":"Guide","title":"Guide","text":"The neuralyze function is used to wipe the memory about the origin of a tensor. This can be helpful when you need to bypass Julia's aliasing checks or ensure that a tensor is treated as an independent array for example in a C call.","category":"page"},{"location":"guide/","page":"Guide","title":"Guide","text":"A = alloc!(buffer, 10, 10)\nAn = neuralyze(A) # Neuralyze tensor A","category":"page"},{"location":"guide/#Complete-Example","page":"Guide","title":"Complete Example","text":"","category":"section"},{"location":"guide/","page":"Guide","title":"Guide","text":"Here is a complete example demonstrating the usage of alloc!, drop!, reset!, and reshape_buf! functions:","category":"page"},{"location":"guide/","page":"Guide","title":"Guide","text":"using Buffers\n\n# Create a buffer\nbuffer = Buffer(1000)\n\n# Allocate tensors\nA = alloc!(buffer, 10, 10)\nB = alloc!(buffer, 20, 5)\n\n# Perform operations on tensors\nrand!(A)\nrand!(B)\n\n# Deallocate tensors\ndrop!(buffer, A)\ndrop!(buffer, B)\n\n# Create a ThreadsBuffer\ntbuffer = ThreadsBuffer(1000)\n\n# Use ThreadsBuffer in a threaded loop\n@sync for i in 1:4\n  Threads.@spawn begin\n    A = alloc!(tbuffer, 10, 10)\n    B = alloc!(tbuffer, 20, 5)\n    drop!(tbuffer, A, B)\n    reset!(tbuffer) # Release the buffer for the current thread\n  end\nend\n\n# Reshape the buffer\nC = reshape_buf!(buffer, 10, 10)\nD = reshape_buf!(buffer, 20, 5, offset=100)","category":"page"},{"location":"guide/","page":"Guide","title":"Guide","text":"For more detailed information, please refer to the other sections of the documentation.","category":"page"},{"location":"release-notes/","page":"Release notes","title":"Release notes","text":"EditURL = \"https://github.com/fkfest/Buffers.jl/blob/master/CHANGELOG.md\"","category":"page"},{"location":"release-notes/#Release-notes","page":"Release notes","title":"Release notes","text":"","category":"section"},{"location":"release-notes/#Unreleased","page":"Release notes","title":"Unreleased","text":"","category":"section"},{"location":"release-notes/#Breaking","page":"Release notes","title":"Breaking","text":"","category":"section"},{"location":"release-notes/#Changed","page":"Release notes","title":"Changed","text":"","category":"section"},{"location":"release-notes/#Added","page":"Release notes","title":"Added","text":"","category":"section"},{"location":"release-notes/#Fixed","page":"Release notes","title":"Fixed","text":"","category":"section"},{"location":"release-notes/#Version-[v0.1.0](https://github.com/fkfest/Buffers.jl/releases/tag/v0.1.0)-2024.12.09","page":"Release notes","title":"Version v0.1.0 - 2024.12.09","text":"","category":"section"},{"location":"release-notes/#Added-2","page":"Release notes","title":"Added","text":"","category":"section"},{"location":"release-notes/","page":"Release notes","title":"Release notes","text":"First release: Buffer, ThreadsBuffer, alloc!, drop!, reset!, release!, @print_buffer_usage","category":"page"},{"location":"buffers/#Buffers","page":"Buffers","title":"Buffers","text":"","category":"section"},{"location":"buffers/","page":"Buffers","title":"Buffers","text":"Buffers","category":"page"},{"location":"buffers/#Buffers","page":"Buffers","title":"Buffers","text":"Buffers module\n\nThis module contains functions to handle buffers.\n\nThe Buffer object is used to store data of type T with an offset, while the ThreadsBuffer object is used to store data of type T with an offset for each thread.\n\nThe buffers are used to store data in a contiguous memory block and to avoid memory allocation in loops. The buffers can be used with alloc! to allocate tensors of given dimensions, drop! to drop tensors from the buffer, and reset! to reset the buffer to the initial state.\n\nAlternativelly, the buffers can be reshaped with reshape_buf! to use the same memory block for different tensors or to allocate tensors with a specific offset.\n\nThe size of the buffer can be extended if necessary, and the buffer can be set to be extendable (default) or not at construction with Buffer or later with set_extendable!.\n\nIn any case, the ::ThreadsBuffer buffers should be released after use with Buffers.release! or reset!.\n\nIf some functions complain about tensors being aliases or if the tensors will be used in C,  the neuralyze function can be used to wipe the memory about the origin of the tensor. Do not use this function if the size of the tensor might be changed in between, i.e., neuralyze the tensor only after all necessary allocations are done.\n\n\n\n\n\n","category":"module"},{"location":"buffers/#Exported-functions","page":"Buffers","title":"Exported functions","text":"","category":"section"},{"location":"buffers/","page":"Buffers","title":"Buffers","text":"Modules = [Buffers]\nPrivate = false\nOrder = [:type, :function, :macro]","category":"page"},{"location":"buffers/#Buffers.Buffer","page":"Buffers","title":"Buffers.Buffer","text":"Buffer{T}\n\nBuffer object to store data of type T with an offset.\n\nThe buffer allocates an extra element at the beginning which is used to check  if the buffer can be extended and to ensure that pointers to the allocated  arrays will never point to the same memory as the buffer.\n\nIf the buffer is used with reshape_buf!, the offset is set to zero.\n\n\n\n\n\n","category":"type"},{"location":"buffers/#Buffers.ThreadsBuffer","page":"Buffers","title":"Buffers.ThreadsBuffer","text":"ThreadsBuffer{T}\n\nBuffer object to store data of type T for each thread.\n\nBy default, the buffer is created for nthreads() threads, i.e., each thread has its own buffer Buffer.\n\nCreate the buffer with ThreadsBuffer{T}(len, nbuf=Threads.nthreads()) and use it with alloc!, drop!, reset!, etc.\n\nwarning: Warning\nAlways reset! or Buffers.release! the buffer after use!\n\nExample\n\njulia> buf = Buffer(10000)\njulia> C = alloc!(buf, 10, 10, 20) # 10x10x20 destination tensor on a single thread\njulia> tbuf = ThreadsBuffer(1000) # 1000 elements buffer for nthreads() threads each\njulia> Threads.@threads for k = 1:20\n          A = alloc!(tbuf, 10, 10) # 10x10 tensor\n          B = alloc!(tbuf, 10, 10) # 10x10 tensor\n          rand!(A)\n          rand!(B)\n          @tensor C[:,:,k][i,j] = A[i,l] * B[l,j]\n          reset!(tbuf)\n        end\n\n\n\n\n\n","category":"type"},{"location":"buffers/#Buffers.alloc!-Tuple{Any, Vararg{Any}}","page":"Buffers","title":"Buffers.alloc!","text":"alloc!(buf, dims...; extend=true)\n\nAllocate tensor of given dimensions in buffer buf.\n\nThe tensor is allocated in the buffer starting at the current offset.   The offset is increased by the length of the tensor.   If extend=true, the buffer is extended if necessary.   For ThreadsBuffer, the tensor is allocated in the buffer of the current thread.\n\nReturn the allocated tensor.\n\njulia> buf = Buffer(100000)\njulia> A = alloc!(buf, 10, 10, 20) # 10x10x20 tensor\njulia> B = alloc!(buf, 10, 10, 10) # 10x10x10 tensor starting after A\njulia> C = alloc!(buf, 10, 20) # 10x20 tensor starting after B\njulia> rand!(B)\njulia> rand!(C)\njulia> An = neuralyze(A) # tensor without origin\njulia> @tensor An[i,j,k] = B[i,j,l] * C[l,k]\n\n\n\n\n\n","category":"method"},{"location":"buffers/#Buffers.drop!-Tuple{Any, Vararg{Any}}","page":"Buffers","title":"Buffers.drop!","text":"drop!(buf, tensor...)\n\nDrop tensor(s) from buffer buf.\n\nOnly last tensors can be dropped.   For ThreadsBuffer, drop tensors from the buffer of the current thread.\n\n\n\n\n\n","category":"method"},{"location":"buffers/#Buffers.isextendable-Tuple{Any}","page":"Buffers","title":"Buffers.isextendable","text":"isextendable(buf)\n\nCheck if buffer buf is extendable.\n\n\n\n\n\n","category":"method"},{"location":"buffers/#Buffers.nbuffers-Tuple{ThreadsBuffer}","page":"Buffers","title":"Buffers.nbuffers","text":"nbuffers(buf::ThreadsBuffer)\n\nReturn the number of buffers in buf::ThreadsBuffer.\n\n\n\n\n\n","category":"method"},{"location":"buffers/#Buffers.neuralyze-Tuple{AbstractArray}","page":"Buffers","title":"Buffers.neuralyze","text":"neuralyze(tensor::AbstractArray)\n\nWipe the memory about origin of tensor.\n\ntensor is a (contiguous!) array that is a (possibly reshaped) view of a larger array.   Return the same tensor pointing to the same memory,    but without the information about the origin.   To be used together with alloc! or reshape_buf! to trick Base.mightalias.\n\nwarning: Warning\nNote that this function is unsafe and should be used with caution! If too much memory is wiped, Julia might garbage-collect the original array and the tensor will point to invalid memory. Also don't use this function if the buffer-size might change in between.\n\ntip: Tip\nOne can use GC.@preserve to prevent the garbage collection of the original array  (however, this shouldn't be necessary).\n\nExample\n\njulia> buf = Buffer(100000)\njulia> A = alloc(buf, 10, 10, 20) # 10x10x20 tensor\njulia> B = alloc(buf, 10, 10, 10) # 10x10x10 tensor starting after A\njulia> C = alloc(buf, 10, 20) # 10x20 tensor starting after B\njulia> rand!(B)\njulia> rand!(C)\njulia> An = neuralyze(A) # tensor without origin but pointing to the same memory\njulia> @tensor An[i,j,k] = B[i,j,l] * C[l,k]\n\n\n\n\n\n","category":"method"},{"location":"buffers/#Buffers.pseudo_alloc!-Tuple{Any, Any, Any}","page":"Buffers","title":"Buffers.pseudo_alloc!","text":"pseudo_alloc!(lenbuf, peakbuf, len)\n\nPseudo allocation function to calculate length for buffer.\n\nThe function is used in combination with @print_buffer_usage.\n\n\n\n\n\n","category":"method"},{"location":"buffers/#Buffers.pseudo_drop!-Tuple{Any, Vararg{Any}}","page":"Buffers","title":"Buffers.pseudo_drop!","text":"pseudo_drop!(lenbuf, lens...)\n\nPseudo drop function to calculate length for buffer.\n\nThe function is used in combination with @print_buffer_usage.\n\n\n\n\n\n","category":"method"},{"location":"buffers/#Buffers.pseudo_reset!-Tuple{Any}","page":"Buffers","title":"Buffers.pseudo_reset!","text":"pseudo_reset!(lenbuf)\n\nPseudo reset function to calculate length for buffer.\n\nThe function is used in combination with @print_buffer_usage.\n\n\n\n\n\n","category":"method"},{"location":"buffers/#Buffers.repair!-Tuple{ThreadsBuffer}","page":"Buffers","title":"Buffers.repair!","text":"repair!(buf::ThreadsBuffer)\n\nRepair ThreadsBuffer buf by releasing all buffers and resetting the pool.\n\nThis function should be used after the threaded loop  if the buffers were not released properly.\n\n\n\n\n\n","category":"method"},{"location":"buffers/#Buffers.reset!-Tuple{Any}","page":"Buffers","title":"Buffers.reset!","text":"reset!(buf)\n\nReset buffer buf to the initial state.   For ThreadsBuffer, reset the buffer of the current thread and release it.\n\n\n\n\n\n","category":"method"},{"location":"buffers/#Buffers.reshape_buf!-Tuple{Any, Vararg{Any}}","page":"Buffers","title":"Buffers.reshape_buf!","text":"reshape_buf!(buf, dims...; offset=0, extend=true)\n\nReshape (part of) a buffer to given dimensions (without copying),   using offset.\n\nFor ThreadsBuffer, reshape the buffer of the current thread.   Call reset!(::ThreadsBuffer) or release! after use.\n\nIt can be used, e.g., for itermediates in tensor contractions.\n\nwarning: Warning\nDo not use this function together with alloc! or drop! on the same buffer!\n\nExample\n\njulia> buf = Buffer(100000)\njulia> A = reshape_buf!(buf, 10, 10, 20) # 10x10x20 tensor\njulia> B = reshape_buf!(buf, 10, 10, 10, offset=2000) # 10x10x10 tensor starting at 2001\njulia> B .= rand(10,10,10)\njulia> C = rand(10,20)\njulia> @tensor A[i,j,k] = B[i,j,l] * C[l,k]\n\n\n\n\n\n","category":"method"},{"location":"buffers/#Buffers.set_extendable!","page":"Buffers","title":"Buffers.set_extendable!","text":"set_extendable!(buf, extend=true)\n\nSet buffer buf to be extendable or not.\n\n\n\n\n\n","category":"function"},{"location":"buffers/#Buffers.used","page":"Buffers","title":"Buffers.used","text":"used(buf)\n\nReturn the number of elements used in buffer buf.\n\nIf the buffer is used with reshape_buf!, -1 is returned.\n\nFor ThreadsBuffer, return the number of elements used in the buffer of the current thread.\n\n\n\n\n\n","category":"function"},{"location":"buffers/#Buffers.with_buffer-Tuple{Function, ThreadsBuffer}","page":"Buffers","title":"Buffers.with_buffer","text":"with_buffer(f::Function, buf::ThreadsBuffer)\n\nExecute function f with buffer buf.\n\nThe buffer is released after the function is executed.\n\nExample\n\njulia> buf = Buffer(10000)\njulia> C = alloc!(buf, 10, 10, 20) # 10x10x20 destination tensor on a single thread\njulia> tbuf = ThreadsBuffer(1000)\njulia> Threads.@threads for k = 1:20\n          with_buffer(tbuf) do bu\n            A = alloc!(bu, 10, 10) # 10x10 tensor\n            B = alloc!(bu, 10, 10) # 10x10 tensor\n            rand!(A)\n            rand!(B)\n            @tensor C[:,:,k][i,j] = A[i,l] * B[l,j]\n          end\n        end\n\n\n\n\n\n","category":"method"},{"location":"buffers/#Buffers.@print_buffer_usage-Tuple{Any, Any}","page":"Buffers","title":"Buffers.@print_buffer_usage","text":"@print_buffer_usage(buf, ex)\n\nPrint buffer buf usage in expression ex.\n\nThe macro generates a body of a function that calculates the length of buffer buf  in expression ex. It is possible to use the macro with multiple buffers, e.g.,  @print_buffer_usage buf1 buf2 begin ... end.\n\nExample\n\n```julia buf = Buffer(100000) @printbufferusage buf begin if true   A = alloc!(buf, 10, 10, 20) else   A = alloc!(buf, 10, 10, 30) end B = alloc!(buf, 10, 10, 10) if true   C = alloc!(buf, 10, 20) else   C = alloc!(buf, 10, 30) end rand!(B) rand!(C) An = neuralyze(A) @tensor An[i,j,k] = B[i,j,l] * C[l,k] drop!(buf, B, C) reset!(buf) end\n\n\n\n\n\n","category":"macro"},{"location":"buffers/#Internal-functions","page":"Buffers","title":"Internal functions","text":"","category":"section"},{"location":"buffers/","page":"Buffers","title":"Buffers","text":"Modules = [Buffers]\nPublic = false\nOrder = [:function, :macro]","category":"page"},{"location":"buffers/#Buffers._buffer_usage-Tuple{Any, Any}","page":"Buffers","title":"Buffers._buffer_usage","text":"_buffer_usage(ex, bufs)\n\nAllocations and deallocations together with corresponding ifs  in expression ex.\n\n\n\n\n\n","category":"method"},{"location":"buffers/#Buffers.current_buffer-Union{Tuple{ThreadsBuffer{T}}, Tuple{T}} where T","page":"Buffers","title":"Buffers.current_buffer","text":"current_buffer(buf::ThreadsBuffer{T})\n\nReturn the buffer of the current thread.\n\nIf the buffer is not available, wait until it is released.\n\n\n\n\n\n","category":"method"},{"location":"buffers/#Buffers.current_buffer_index-Tuple{ThreadsBuffer}","page":"Buffers","title":"Buffers.current_buffer_index","text":"current_buffer_index(buf::ThreadsBuffer)\n\nReturn the index of the buffer of the current thread.\n\nIf the buffer is not available, wait until it is released.\n\n\n\n\n\n","category":"method"},{"location":"buffers/#Buffers.iscontiguous_tensor-Tuple{AbstractArray}","page":"Buffers","title":"Buffers.iscontiguous_tensor","text":"iscontiguous_tensor(tensor::AbstractArray)\n\nCheck if tensor is contiguous.\n\nReturn true if tensor is a Vector or a SubArray that is contiguous.\n\n\n\n\n\n","category":"method"},{"location":"buffers/#Buffers.release!-Tuple{ThreadsBuffer}","page":"Buffers","title":"Buffers.release!","text":"release!(buf::ThreadsBuffer)\n\nRelease buffer of the current thread.\n\n\n\n\n\n","category":"method"},{"location":"prealloc/#Preallocating-Buffers-with-@print_buffer_usage","page":"Preallocation","title":"Preallocating Buffers with @print_buffer_usage","text":"","category":"section"},{"location":"prealloc/","page":"Preallocation","title":"Preallocation","text":"The @print_buffer_usage macro in Buffers.jl is a useful tool for determining the memory requirements of a buffer within a given block of code. This allows you to preallocate the buffer with the appropriate size, ensuring efficient memory usage and avoiding unnecessary reallocations.","category":"page"},{"location":"prealloc/#Usage","page":"Preallocation","title":"Usage","text":"","category":"section"},{"location":"prealloc/","page":"Preallocation","title":"Preallocation","text":"To use the @print_buffer_usage macro, simply wrap your code block with the macro and specify the buffer(s) you want to analyze. The macro will print the function body to determine the peak memory usage of the buffer(s) within the code block.","category":"page"},{"location":"prealloc/#Example","page":"Preallocation","title":"Example","text":"","category":"section"},{"location":"prealloc/","page":"Preallocation","title":"Preallocation","text":"using Buffers\n\nbuf = Buffer(100000)\n@print_buffer_usage buf begin\n  if small\n    A = alloc!(buf, 10, 10, 20)\n  else\n    A = alloc!(buf, 10, 10, 30)\n  end\n  B = alloc!(buf, 10, 10, 10)\n  if small\n    C = alloc!(buf, 10, 20)\n  else\n    C = alloc!(buf, 10, 30)\n  end\n  rand!(B)\n  rand!(C)\n  An = neuralyze(A)\n  @tensor An[i,j,k] = B[i,j,l] * C[l,k]\n  drop!(buf, B, C)\n  reset!(buf)\nend","category":"page"},{"location":"prealloc/","page":"Preallocation","title":"Preallocation","text":"In this example, the @print_buffer_usage macro will analyze the memory usage of the buffer buf within the code block and print the body of the function which can be used to preallocate the buffer with the appropriate size based on the peak memory usage.","category":"page"},{"location":"prealloc/","page":"Preallocation","title":"Preallocation","text":"# Function to calculate length for buffer(s) buf\n# autogenerated by @print_buffer_usage\n=============================================\nquote\n    lenbuf = Ref(0)\n    peakbuf = Ref(0)\n    if small\n        A = pseudo_alloc!(lenbuf, peakbuf, 10 * 10 * 20)\n    else\n        A = pseudo_alloc!(lenbuf, peakbuf, 10 * 10 * 30)\n    end\n    B = pseudo_alloc!(lenbuf, peakbuf, 10 * 10 * 10)\n    if small\n        C = pseudo_alloc!(lenbuf, peakbuf, 10 * 20)\n    else\n        C = pseudo_alloc!(lenbuf, peakbuf, 10 * 30)\n    end\n    pseudo_drop!(lenbuf, B, C)\n    pseudo_reset!(lenbuf)\n    return peakbuf[]\nend\n=============================================","category":"page"},{"location":"prealloc/","page":"Preallocation","title":"Preallocation","text":"Wrap the autogenerated function body to a function and use it to preallocate the buffer with the correct size.","category":"page"},{"location":"prealloc/","page":"Preallocation","title":"Preallocation","text":"function preallocate_buffer(small)\n    lenbuf = Ref(0)\n    peakbuf = Ref(0)\n    if small\n        A = pseudo_alloc!(lenbuf, peakbuf, 10 * 10 * 20)\n    else\n        A = pseudo_alloc!(lenbuf, peakbuf, 10 * 10 * 30)\n    end\n    B = pseudo_alloc!(lenbuf, peakbuf, 10 * 10 * 10)\n    if small\n        C = pseudo_alloc!(lenbuf, peakbuf, 10 * 20)\n    else\n        C = pseudo_alloc!(lenbuf, peakbuf, 10 * 30)\n    end\n    pseudo_drop!(lenbuf, B, C)\n    pseudo_reset!(lenbuf)\n    return peakbuf[]\nend\n\nbuf = Buffer(preallocate_buffer(true))","category":"page"},{"location":"prealloc/#Multiple-Buffers","page":"Preallocation","title":"Multiple Buffers","text":"","category":"section"},{"location":"prealloc/","page":"Preallocation","title":"Preallocation","text":"The @print_buffer_usage macro can also be used with multiple buffers. Simply list the buffers you want to analyze before the code block.","category":"page"},{"location":"prealloc/","page":"Preallocation","title":"Preallocation","text":"using Buffers\n\nbuf1 = Buffer(100000)\nbuf2 = Buffer(50000)\n@print_buffer_usage buf1 buf2 begin\n    A = alloc!(buf1, 10, 10, 20)\n    B = alloc!(buf2, 20, 5)\n    rand!(A)\n    rand!(B)\n    drop!(buf1, A)\n    drop!(buf2, B)\n    reset!(buf1)\n    reset!(buf2)\nend","category":"page"},{"location":"prealloc/","page":"Preallocation","title":"Preallocation","text":"This will print the peak memory usage for both buf1 and buf2.","category":"page"},{"location":"prealloc/","page":"Preallocation","title":"Preallocation","text":"By using the @print_buffer_usage macro, you can ensure that your buffers are preallocated with the correct size, leading to more efficient memory management and improved performance in your applications.","category":"page"},{"location":"tensor_contractions/#Tensor-Contractions-Using-Buffers.jl","page":"Tensor contractions","title":"Tensor Contractions Using Buffers.jl","text":"","category":"section"},{"location":"tensor_contractions/","page":"Tensor contractions","title":"Tensor contractions","text":"This guide demonstrates how to perform efficient tensor contractions using Buffers.jl with two separate buffers. By managing input and output tensors in different buffers, you can optimize memory usage and improve performance, especially in high-performance computing applications.","category":"page"},{"location":"tensor_contractions/#Overview","page":"Tensor contractions","title":"Overview","text":"","category":"section"},{"location":"tensor_contractions/","page":"Tensor contractions","title":"Tensor contractions","text":"When performing tensor contractions, it's common to have intermediate tensors and output tensors. Allocating these tensors in separate buffers allows for better control over memory and can prevent memory fragmentation. As a simple rule of thumb, for each tensor contraction operation, allocate input tensors in one buffer and the output tensor in another buffer. This allows you to free memory efficiently after each operation.","category":"page"},{"location":"tensor_contractions/#Example-Use-Case","page":"Tensor contractions","title":"Example Use Case","text":"","category":"section"},{"location":"tensor_contractions/","page":"Tensor contractions","title":"Tensor contractions","text":"In this example use case, we will perform tensor contraction operations using Buffers.jl. Consider the following tensor contraction operation:","category":"page"},{"location":"tensor_contractions/","page":"Tensor contractions","title":"Tensor contractions","text":"E_ij^kl = A_ij^ab B_ab^cd C_cd^ef D_ef^kl","category":"page"},{"location":"tensor_contractions/","page":"Tensor contractions","title":"Tensor contractions","text":"where A, B, C, and D are input tensors and E is the output tensor. We will perform binary tensor contractions using the @tensor macro from the TensorOperations.jl package. We will allocate A and B in one buffer, AB and C in another buffer, ABC and D in the first buffer and so on.","category":"page"},{"location":"tensor_contractions/#Setting-Up-Buffers","page":"Tensor contractions","title":"Setting Up Buffers","text":"","category":"section"},{"location":"tensor_contractions/","page":"Tensor contractions","title":"Tensor contractions","text":"First, create two buffers (buf1) and (buf2) which will be used interchangeably for input and output tensors. The size of each buffer should be based on the memory requirements of your tensors (use @print_buffer_usage to calculate the sizes or let the buffers grow automatically).","category":"page"},{"location":"tensor_contractions/","page":"Tensor contractions","title":"Tensor contractions","text":"using Buffers\n\n# Create buffers with appropriate sizes\nbuf1 = Buffer(100000)\nbuf2 = Buffer(50000)","category":"page"},{"location":"tensor_contractions/#Performing-Tensor-Contractions","page":"Tensor contractions","title":"Performing Tensor Contractions","text":"","category":"section"},{"location":"tensor_contractions/","page":"Tensor contractions","title":"Tensor contractions","text":"Allocate tensors in the input buffer for your computations. The input tensors should be allocated in the buffer buf1, and the output tensor should be allocated in the buffer buf2.","category":"page"},{"location":"tensor_contractions/","page":"Tensor contractions","title":"Tensor contractions","text":"# Allocate input tensors in the input buffer\nA = alloc!(buf1, ni, nj, na, nb)   # ni*nj*na*nb tensor\nB = alloc!(buf1, na, nb, nc, nd)   # na*nb*nc*nd tensor\nAB = alloc!(buf2, ni, nj, nc, nd)  # ni*nj*nc*nd tensor\n\n# Initialize tensors with random values or data\nrand!(B)\nrand!(C)","category":"page"},{"location":"tensor_contractions/","page":"Tensor contractions","title":"Tensor contractions","text":"Perform the tensor contraction using the allocated input tensors and drop the input tensors from the input buffer.","category":"page"},{"location":"tensor_contractions/","page":"Tensor contractions","title":"Tensor contractions","text":"# Perform tensor contraction (example using @tensor macro from TensorOperations.jl)\nusing TensorOperations\n\n@tensor AB[i, j, c, d] = A[i, j, a, b] * B[a, b, c, d]\ndrop!(buf1, A, B)","category":"page"},{"location":"tensor_contractions/","page":"Tensor contractions","title":"Tensor contractions","text":"Allocate the new input tensor C in the buffer buf2 and the new output tensor ABC in the buffer buf1 and perform the next tensor contraction, etc.","category":"page"},{"location":"tensor_contractions/","page":"Tensor contractions","title":"Tensor contractions","text":"# Allocate new input tensor in the output buffer\nC = alloc!(buf2, nc, nd, ne, nf)   # nc*nd*ne*nf tensor\nABC = alloc!(buf1, ni, nj, ne, nf) # ni*nj*ne*nf tensor\n@tensor ABC[i, j, e, f] = AB[i, j, c, d] * C[c, d, e, f]\ndrop!(buf2, C, AB)\nD = alloc!(buf1, ne, nf, nk, nl)   # ne*nf*nk*nl tensor\nE = alloc!(buf2, ni, nj, nk, nl)   # ni*nj*nk*nl output tensor\n@tensor E[i, j, k, l] = ABC[i, j, e, f] * D[e, f, k, l]\ndrop!(buf1, D, ABC)","category":"page"},{"location":"#Buffers.jl-Documentation","page":"Home","title":"Buffers.jl Documentation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Buffers.jl is a Julia package designed to provide efficient and flexible buffer management for various data types. It offers a range of functionalities to handle dynamic data storage needs, including resizing, appending, and accessing elements with minimal overhead. This package is particularly useful for applications that require high-performance data manipulation and storage solutions.","category":"page"},{"location":"#Key-Features","page":"Home","title":"Key Features","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Dynamic Buffer Management: Efficiently allocate and deallocate memory for multidimensional arrays.\nThread-Safe Operations: Support for multi-threaded environments with ThreadsBuffer.\nHigh Performance: Minimize overhead and maximize performance for data-intensive applications.\nFlexible Usage: Easily reshape buffers and manage memory without frequent reallocations.\nAdvanced Memory Manipulation: Use neuralyze to manipulate tensor aliasing and optimize performance in complex computations.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Buffers.jl is ideal for scientific computing, data analysis, and any application where efficient memory management is crucial.","category":"page"},{"location":"#Installation","page":"Home","title":"Installation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"To install Buffers.jl, use the Julia package manager. From the Julia REPL, type ] to enter the Pkg REPL mode and run:","category":"page"},{"location":"","page":"Home","title":"Home","text":"pkg> add Buffers","category":"page"},{"location":"#Usage","page":"Home","title":"Usage","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"To use Buffers.jl, import the package and create a buffer object:","category":"page"},{"location":"","page":"Home","title":"Home","text":"using Buffers\n\nbuffer = Buffer{Int}()","category":"page"},{"location":"","page":"Home","title":"Home","text":"The size of the buffer is automatically adjusted based on the elements added to it. However, to avoid reallocation you can specify the initial size of the buffer (number of elements) when creating it:","category":"page"},{"location":"","page":"Home","title":"Home","text":"buffer = Buffer(100)","category":"page"},{"location":"","page":"Home","title":"Home","text":"You can then perform various operations on the buffer, such as allocating memory and clearing the buffer:","category":"page"},{"location":"","page":"Home","title":"Home","text":"A = alloc!(buffer, 10, 10)\ndrop!(buffer, A)\nreset!(buffer) # To clear all tensors from the buffer","category":"page"},{"location":"","page":"Home","title":"Home","text":"For more detailed usage instructions and examples, please refer to the following sections of the documentation.","category":"page"}]
}
